Usage: polybench [OPTION]... --path [FILE]

This launcher is designed for benchmarking Truffle languages.
It is typically not invoked directly, but through mx (which sets up the classpath, arguments, etc.).
Run "mx polybench --help" for more details.

The launcher repeatedly executes the source file specified by its path.
The number of hot and warmup iterations can be specified
either explicitly or determined via interop from the evaluated source value
by reading or executing members "iterations" and "warmupIterations".

The launcher generates structured output containing information
on individual iterations as well as the benchmark summary.

The following options are accepted:

  --path <script>
       The benchmark file to run. The corresponding Truffle language
       for the benchmark should be available on the classpath.

  --class-name <class>
       The class name of an Espresso benchmark inside the jar specified 
       by path (only relevant when executing Java workloads).

  --metric [peak-time | compilation-time | partial-evaluation-time | one-shot | 
            metaspace-memory | application-memory | none | ...] (default peak-time)
       The metric to measure. Additional options may be available if any
       additional subclasses of org.graalvm.polybench.Metric are present
       on the classpath; for example, a custom metric MyMeasurementMetric on the
       classpath can be selected using --metric my-measurement.
  
  -w <N>
       The number of warmup iterations.

  -i <N>
       The number of hot iterations.

  --eval-source-only
       Indicates that the source file will be evaluated only, and the benchmark 
       execution will be skipped. It can be useful for parsing time benchmarking 
       or for using the debug auxiliary engine cache. N.B. This option can also 
       be specified for each run in a multi-context benchmarking. See below.

Multi-context benchmarking

This launcher allows for multi-context benchmarking consisting of several consecutive 
benchmark runs. Each run is executed using a new context instance, while all contexts 
can share a single engine. A multi-context benchmarking can be configured using the 
following options:

  --multi-context-runs=<N>
       The number of runs

  --shared-engine=[true | false]
       Indicates whether the contexts will share a single engine. If false, the context 
       of each run will have a separate engine, otherwise all contexts will share a single 
       engine. It is possible to specify different arguments for each run. The pattern of a 
       run specific argument is:

 <arg_name>.<run>=<value>

Staging the launcher to a target language

This launcher allows transpiling of the main harness loop into a target language,
facilitating comparisons with other language implementations. The staged program contains:
* The main harness loop.
* All methods and classes needed for the execution of the main harness loop (e.g. requested
  Metric class, required Summary class).
* The benchmark source code (copied from the benchmark source file).
This feature is provided on a best-effort basis - not all Truffle languages are supported,
and not all metrics are supported for supported languages. The staging can be configured
using the following options:

 --stage-to-language <LANGUAGE>
       The language into which to stage the main harness loop. If provided staging is enabled
       and a staged harness loop will be produced in the target language instead of harness
       execution (the default behaviour). Currently supported values are: python.

 --stage-to-file <PATH>
       Path to file into which to write the staged program. Must be provided if staging
       is enabled.

 --log-staged-program <BOOL>
       Whether to dump the staged program into stdout. Defaults to: FALSE.

 --run-staged-program-with <LAUNCHER>
       Execute the staged program after transpiling using the provided launcher. If this option
       is not set then the staged program will not be executed. This option allows the entire
       pipeline (transpiling and execution) in the same command. You can always execute the staged
       program on your own afterwards.
 
 
Examples:
 
  A single run with 5 warmup and 10 hot iterations
     polybench --path bench.so -w 5 -i 10
 
  Two runs with 5 warmup and 10 hot iterations each (no shared engine)
     polybench --path bench.so --multi-context-runs=2 -w 5 -i 10
 
  Two runs with 5 warmup and 10 hot iterations each with a shared engine
     polybench --path bench.so --multi-context-runs=2 --shared-engine=true -w 5 -i 10
 
  Storing the auxiliary engine cache and skipping the benchmark
     polybench --path bench.so --eval-source-only=true --experimental-options 
               --engine.CacheStore=test.image --engine.CacheCompile=aot
 
  Loading the auxiliary engine cache and running the benchmark
     polybench --path bench.so --experimental-options 
               --engine.CacheLoad=test.image -w 0 -i 10
 
  Using the debug engine cache. The first run only evaluates the source and stores the debug 
  engine cache, while the second run executes the benchmark with the cache.
     polybench --path bench.so --multi-context-runs=2 --eval-source-only.0=true 
               -w 0 -i 10 --experimental-options --engine.DebugCacheCompile.0=aot 
               --engine.DebugCacheStore.0=true --engine.DebugCacheStore.1=false --engine.DebugCacheLoad.1=true
 
  Reporting the source evaluation time only for 10 runs
     polybench --path bench.so --multi-context-runs=10 --eval-source-only=true

  Staging the main harness loop into Python:
     polybench --path bench.py --stage-to-language python --stage-to-file staged-bench.py

  Staging the main harness loop into Python, with the staged program logged to stdout
  and immediate execution:
     polybench --path bench.py --stage-to-language python --stage-to-file staged-bench.py
               --log-staged-program True --run-staged-program-with python3
 
 